{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Detection and Tracking\n",
    "Based on: https://github.com/emasterclassacademy/Single-Multiple-Custom-Object-Detection-and-Tracking\n",
    "\n",
    "Using OpenCV and EuclidanDistTracker. Machine Learning approach. \n",
    "\n",
    "Object Detection - YOLO\n",
    "- Will be applied to each and every frame\n",
    "\n",
    "Object Tracking - DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tf_cuda(): \n",
    "    import tensorflow as tf\n",
    "    return len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "def check_cv2_cuda():\n",
    "    import cv2\n",
    "    import re\n",
    "    cv_info = [re.sub('\\s+', ' ', ci.strip()) for ci in cv2.getBuildInformation().strip().split('\\n')\n",
    "                if len(ci) > 0 and re.search(r'(nvidia*:?)|(cuda*:)|(cudnn*:)', ci.lower()) is not None]\n",
    "\n",
    "    return len(cv_info) > 0\n",
    "\n",
    "def download_weights():\n",
    "    # download weights of not present\n",
    "    import urllib.request\n",
    "    import os\n",
    "    if not os.path.exists(\"weights/yolov3.weights\"):\n",
    "        print(\"Downloading weights...\")\n",
    "        urllib.request.urlretrieve(settings.weight_urls, \"weights/yolov3.weights\")\n",
    "    else:\n",
    "        print(\"Weights already downloaded\")\n",
    "\n",
    "    if not os.path.exists(\"weights/yolov3-tiny.weights\"):\n",
    "        print(\"Downloading tiny weights...\")\n",
    "        urllib.request.urlretrieve(settings.tiny_weight_urls, \"weights/yolov3-tiny.weights\")\n",
    "    else:\n",
    "        print(\"Tiny Weights already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU is available\n",
      "CV2 GPU is available\n",
      "Downloading weights...\n",
      "Downloading tiny weights...\n",
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " yolo_darknet (Functional)      ((None, None, None,  40620640    ['input[0][0]']                  \n",
      "                                 256),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 512),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 1024))                                                           \n",
      "                                                                                                  \n",
      " yolo_conv_0 (Functional)       (None, None, None,   11024384    ['yolo_darknet[0][2]']           \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " yolo_conv_1 (Functional)       (None, None, None,   2957312     ['yolo_conv_0[0][0]',            \n",
      "                                256)                              'yolo_darknet[0][1]']           \n",
      "                                                                                                  \n",
      " yolo_conv_2 (Functional)       (None, None, None,   741376      ['yolo_conv_1[0][0]',            \n",
      "                                128)                              'yolo_darknet[0][0]']           \n",
      "                                                                                                  \n",
      " yolo_output_0 (Functional)     (None, None, None,   4984063     ['yolo_conv_0[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_1 (Functional)     (None, None, None,   1312511     ['yolo_conv_1[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_2 (Functional)     (None, None, None,   361471      ['yolo_conv_2[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_boxes_0 (Lambda)          ((None, None, None,  0           ['yolo_output_0[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_1 (Lambda)          ((None, None, None,  0           ['yolo_output_1[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_2 (Lambda)          ((None, None, None,  0           ['yolo_output_2[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_nms (Lambda)              ((None, 100, 4),     0           ['yolo_boxes_0[0][0]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][1]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][2]',           \n",
      "                                 (None,))                         'yolo_boxes_1[0][0]',           \n",
      "                                                                  'yolo_boxes_1[0][1]',           \n",
      "                                                                  'yolo_boxes_1[0][2]',           \n",
      "                                                                  'yolo_boxes_2[0][0]',           \n",
      "                                                                  'yolo_boxes_2[0][1]',           \n",
      "                                                                  'yolo_boxes_2[0][2]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "model created\n",
      "weights loaded\n",
      "sanity check passed\n",
      "weights saved\n",
      "Model: \"yolov3_tiny\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " yolo_darknet (Functional)      ((None, None, None,  6298480     ['input[0][0]']                  \n",
      "                                 256),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 1024))                                                           \n",
      "                                                                                                  \n",
      " yolo_conv_0 (Functional)       (None, None, None,   263168      ['yolo_darknet[0][1]']           \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " yolo_conv_1 (Functional)       (None, None, None,   33280       ['yolo_conv_0[0][0]',            \n",
      "                                384)                              'yolo_darknet[0][0]']           \n",
      "                                                                                                  \n",
      " yolo_output_0 (Functional)     (None, None, None,   1312511     ['yolo_conv_0[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_1 (Functional)     (None, None, None,   951295      ['yolo_conv_1[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_boxes_0 (Lambda)          ((None, None, None,  0           ['yolo_output_0[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_1 (Lambda)          ((None, None, None,  0           ['yolo_output_1[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_nms (Lambda)              ((None, 100, 4),     0           ['yolo_boxes_0[0][0]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][1]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][2]',           \n",
      "                                 (None,))                         'yolo_boxes_1[0][0]',           \n",
      "                                                                  'yolo_boxes_1[0][1]',           \n",
      "                                                                  'yolo_boxes_1[0][2]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,858,734\n",
      "Trainable params: 8,852,366\n",
      "Non-trainable params: 6,368\n",
      "__________________________________________________________________________________________________\n",
      "model created\n",
      "weights loaded\n",
      "sanity check passed\n",
      "weights saved\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import settings\n",
    "\n",
    "settings.init()\n",
    "\n",
    "# Check if we have a GPU support TF\n",
    "if check_tf_cuda():\n",
    "    print(\"TensorFlow GPU is available\")\n",
    "else:\n",
    "    print(\"TensorFlow GPU is NOT available\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check if we have a GPU support OpenCV\n",
    "if check_cv2_cuda():\n",
    "    print(\"CV2 GPU is available\")\n",
    "else:\n",
    "    print(\"CV2 GPU is NOT available\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# check if weights are downloaded\n",
    "download_weights()\n",
    "\n",
    "# check if weights have been converted\n",
    "# Convert the weights to TensorFlow\n",
    "from convert import convert\n",
    "convert()\n",
    "convert(tiny=True, weights=\"weights/yolov3-tiny.weights\", output=\"weights/yolov3-tiny.tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region of interest:  (464, 399, 618, 240)\n",
      "Time required to align video from: 0.017964839935302734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\.conda\\envs\\opencv-tracking-v4\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to predict: 1.5052995681762695\n",
      "Time required to encode: 2.576633930206299\n",
      "Time required to run non maxima suppression: 2.5776350498199463\n",
      "Time required for tracker to update: 2.5776350498199463\n",
      "Time required to draw results for each track: 2.5776350498199463\n",
      "Time required to align video from: 0.02399730682373047\n",
      "Time required to predict: 1.120332956314087\n",
      "Time required to encode: 1.5877008438110352\n",
      "Time required to run non maxima suppression: 1.5877008438110352\n",
      "Time required for tracker to update: 1.588670015335083\n",
      "Time required to draw results for each track: 1.588670015335083\n",
      "Time required to align video from: 0.016026020050048828\n",
      "Time required to predict: 1.0610721111297607\n",
      "Time required to encode: 1.5211262702941895\n",
      "Time required to run non maxima suppression: 1.5220956802368164\n",
      "Time required for tracker to update: 1.5220956802368164\n",
      "Time required to draw results for each track: 1.5231027603149414\n",
      "Time required to align video from: 0.01699995994567871\n",
      "Time required to predict: 1.0994665622711182\n",
      "Time required to encode: 1.6116697788238525\n",
      "Time required to run non maxima suppression: 1.6116697788238525\n",
      "Time required for tracker to update: 1.6126441955566406\n",
      "Time required to draw results for each track: 1.613645076751709\n",
      "Time required to align video from: 0.015019655227661133\n",
      "Time required to predict: 1.0865085124969482\n",
      "Time required to encode: 1.552184820175171\n",
      "Time required to run non maxima suppression: 1.552184820175171\n",
      "Time required for tracker to update: 1.5531580448150635\n",
      "Time required to draw results for each track: 1.5531580448150635\n",
      "Time required to align video from: 0.014999628067016602\n",
      "Time required to predict: 1.13301420211792\n",
      "Time required to encode: 1.6025919914245605\n",
      "Time required to run non maxima suppression: 1.6025919914245605\n",
      "Time required for tracker to update: 1.6034178733825684\n",
      "Time required to draw results for each track: 1.6034178733825684\n",
      "Time required to align video from: 0.017994165420532227\n",
      "Time required to predict: 1.1567046642303467\n",
      "Time required to encode: 1.6634581089019775\n",
      "Time required to run non maxima suppression: 1.6644654273986816\n",
      "Time required for tracker to update: 1.665436029434204\n",
      "Time required to draw results for each track: 1.665436029434204\n",
      "Time required to align video from: 0.015903472900390625\n",
      "Time required to predict: 1.1307997703552246\n",
      "Time required to encode: 1.6227800846099854\n",
      "Time required to run non maxima suppression: 1.6227800846099854\n",
      "Time required for tracker to update: 1.6237819194793701\n",
      "Time required to draw results for each track: 1.6247813701629639\n",
      "Time required to align video from: 0.014998674392700195\n",
      "Time required to predict: 1.1423699855804443\n",
      "Time required to encode: 1.6273400783538818\n",
      "Time required to run non maxima suppression: 1.6273400783538818\n",
      "Time required for tracker to update: 1.6283767223358154\n",
      "Time required to draw results for each track: 1.6283767223358154\n",
      "Time required to align video from: 0.016036272048950195\n",
      "Time required to predict: 1.1668274402618408\n",
      "Time required to encode: 1.6771409511566162\n",
      "Time required to run non maxima suppression: 1.6771409511566162\n",
      "Time required for tracker to update: 1.678434133529663\n",
      "Time required to draw results for each track: 1.678434133529663\n",
      "Time required to align video from: 0.01599407196044922\n",
      "Time required to predict: 1.1548316478729248\n",
      "Time required to encode: 1.6448309421539307\n",
      "Time required to run non maxima suppression: 1.6448309421539307\n",
      "Time required for tracker to update: 1.6458311080932617\n",
      "Time required to draw results for each track: 1.6458311080932617\n",
      "Time required to align video from: 0.01602959632873535\n",
      "Time required to predict: 1.1877167224884033\n",
      "Time required to encode: 1.6878275871276855\n",
      "Time required to run non maxima suppression: 1.6878275871276855\n",
      "Time required for tracker to update: 1.6888551712036133\n",
      "Time required to draw results for each track: 1.6888551712036133\n",
      "Time required to align video from: 0.01702570915222168\n",
      "Time required to predict: 1.1928045749664307\n",
      "Time required to encode: 1.193803310394287\n",
      "Time required to run non maxima suppression: 1.193803310394287\n",
      "Time required for tracker to update: 1.194805383682251\n",
      "Time required to draw results for each track: 1.194805383682251\n",
      "Time required to align video from: 0.016024351119995117\n",
      "Time required to predict: 1.204545259475708\n",
      "Time required to encode: 1.2065706253051758\n",
      "Time required to run non maxima suppression: 1.2065706253051758\n",
      "Time required for tracker to update: 1.2065706253051758\n",
      "Time required to draw results for each track: 1.2065706253051758\n",
      "Time required to align video from: 0.016029834747314453\n",
      "Time required to predict: 1.2261829376220703\n",
      "Time required to encode: 1.227180004119873\n",
      "Time required to run non maxima suppression: 1.227180004119873\n",
      "Time required for tracker to update: 1.2282040119171143\n",
      "Time required to draw results for each track: 1.2282040119171143\n",
      "Time required to align video from: 0.01602768898010254\n",
      "Time required to predict: 1.2180781364440918\n",
      "Time required to encode: 1.7757892608642578\n",
      "Time required to run non maxima suppression: 1.7767910957336426\n",
      "Time required for tracker to update: 1.7777917385101318\n",
      "Time required to draw results for each track: 1.7777917385101318\n",
      "Time required to align video from: 0.014992952346801758\n",
      "Time required to predict: 1.2374930381774902\n",
      "Time required to encode: 1.7625436782836914\n",
      "Time required to run non maxima suppression: 1.7625436782836914\n",
      "Time required for tracker to update: 1.7635457515716553\n",
      "Time required to draw results for each track: 1.7635457515716553\n",
      "Time required to align video from: 0.015993833541870117\n",
      "Time required to predict: 1.4163951873779297\n",
      "Time required to encode: 1.9613888263702393\n",
      "Time required to run non maxima suppression: 1.9613888263702393\n",
      "Time required for tracker to update: 1.9623901844024658\n",
      "Time required to draw results for each track: 1.9633934497833252\n",
      "Time required to align video from: 0.01498723030090332\n",
      "Time required to predict: 1.4117529392242432\n",
      "Time required to encode: 2.0144898891448975\n",
      "Time required to run non maxima suppression: 2.0144898891448975\n",
      "Time required for tracker to update: 2.0154895782470703\n",
      "Time required to draw results for each track: 2.0154895782470703\n",
      "Time required to align video from: 0.017033815383911133\n",
      "Time required to predict: 1.2515864372253418\n",
      "Time required to encode: 1.2545878887176514\n",
      "Time required to run non maxima suppression: 1.2545878887176514\n",
      "Time required for tracker to update: 1.2545878887176514\n",
      "Time required to draw results for each track: 1.2545878887176514\n",
      "Time required to align video from: 0.016998767852783203\n",
      "Time required to predict: 1.221083641052246\n",
      "Time required to encode: 1.2230565547943115\n",
      "Time required to run non maxima suppression: 1.2230565547943115\n",
      "Time required for tracker to update: 1.2230565547943115\n",
      "Time required to draw results for each track: 1.2230565547943115\n",
      "Time required to align video from: 0.01600027084350586\n",
      "Time required to predict: 1.3180453777313232\n",
      "Time required to encode: 1.9349277019500732\n",
      "Time required to run non maxima suppression: 1.9349277019500732\n",
      "Time required for tracker to update: 1.9358975887298584\n",
      "Time required to draw results for each track: 1.9358975887298584\n",
      "Time required to align video from: 0.015998363494873047\n",
      "Time required to predict: 1.2549428939819336\n",
      "Time required to encode: 1.7839980125427246\n",
      "Time required to run non maxima suppression: 1.7849724292755127\n",
      "Time required for tracker to update: 1.7859735488891602\n",
      "Time required to draw results for each track: 1.7859735488891602\n",
      "Time required to align video from: 0.01599907875061035\n",
      "Time required to predict: 1.4159581661224365\n",
      "Time required to encode: 1.9639549255371094\n",
      "Time required to run non maxima suppression: 1.9639549255371094\n",
      "Time required for tracker to update: 1.9649567604064941\n",
      "Time required to draw results for each track: 1.9649567604064941\n",
      "Time required to align video from: 0.015995502471923828\n",
      "Time required to predict: 1.4151442050933838\n",
      "Time required to encode: 2.0653862953186035\n",
      "Time required to run non maxima suppression: 2.0653862953186035\n",
      "Time required for tracker to update: 2.0663866996765137\n",
      "Time required to draw results for each track: 2.0663866996765137\n",
      "Time required to align video from: 0.019999027252197266\n",
      "Time required to predict: 1.264538049697876\n",
      "Time required to encode: 1.8125383853912354\n",
      "Time required to run non maxima suppression: 1.8125383853912354\n",
      "Time required for tracker to update: 1.8135104179382324\n",
      "Time required to draw results for each track: 1.8135104179382324\n",
      "Time required to align video from: 0.016997575759887695\n",
      "Time required to predict: 1.2683756351470947\n",
      "Time required to encode: 1.270373821258545\n",
      "Time required to run non maxima suppression: 1.270373821258545\n",
      "Time required for tracker to update: 1.270373821258545\n",
      "Time required to draw results for each track: 1.270373821258545\n",
      "Time required to align video from: 0.016988039016723633\n",
      "Time required to predict: 1.363715648651123\n",
      "Time required to encode: 2.019434690475464\n",
      "Time required to run non maxima suppression: 2.019434690475464\n",
      "Time required for tracker to update: 2.0204315185546875\n",
      "Time required to draw results for each track: 2.0204315185546875\n",
      "Time required to align video from: 0.017018795013427734\n",
      "Time required to predict: 1.25449538230896\n",
      "Time required to encode: 1.256493091583252\n",
      "Time required to run non maxima suppression: 1.256493091583252\n",
      "Time required for tracker to update: 1.256493091583252\n",
      "Time required to draw results for each track: 1.256493091583252\n",
      "Time required to align video from: 0.016986608505249023\n",
      "Time required to predict: 1.2592425346374512\n",
      "Time required to encode: 1.2602412700653076\n",
      "Time required to run non maxima suppression: 1.2602412700653076\n",
      "Time required for tracker to update: 1.2612411975860596\n",
      "Time required to draw results for each track: 1.2612411975860596\n",
      "Time required to align video from: 0.01599431037902832\n",
      "Time required to predict: 1.297318458557129\n",
      "Time required to encode: 1.2993113994598389\n",
      "Time required to run non maxima suppression: 1.2993113994598389\n",
      "Time required for tracker to update: 1.2993113994598389\n",
      "Time required to draw results for each track: 1.2993113994598389\n",
      "Time required to align video from: 0.016034841537475586\n",
      "Time required to predict: 1.3031883239746094\n",
      "Time required to encode: 1.8802075386047363\n",
      "Time required to run non maxima suppression: 1.8811917304992676\n",
      "Time required for tracker to update: 1.8821930885314941\n",
      "Time required to draw results for each track: 1.8821930885314941\n",
      "Time required to align video from: 0.016002416610717773\n",
      "Time required to predict: 1.5193431377410889\n",
      "Time required to encode: 2.1603479385375977\n",
      "Time required to run non maxima suppression: 2.1603479385375977\n",
      "Time required for tracker to update: 2.1613481044769287\n",
      "Time required to draw results for each track: 2.1613481044769287\n",
      "Time required to align video from: 0.015995264053344727\n",
      "Time required to predict: 1.499915361404419\n",
      "Time required to encode: 2.2364003658294678\n",
      "Time required to run non maxima suppression: 2.2364003658294678\n",
      "Time required for tracker to update: 2.238401174545288\n",
      "Time required to draw results for each track: 2.238401174545288\n",
      "Time required to align video from: 0.016994476318359375\n",
      "Time required to predict: 1.28995680809021\n",
      "Time required to encode: 1.870980978012085\n",
      "Time required to run non maxima suppression: 1.8719837665557861\n",
      "Time required for tracker to update: 1.8719837665557861\n",
      "Time required to draw results for each track: 1.8729844093322754\n",
      "Time required to align video from: 0.01598978042602539\n",
      "Time required to predict: 1.4510142803192139\n",
      "Time required to encode: 2.069058656692505\n",
      "Time required to run non maxima suppression: 2.069058656692505\n",
      "Time required for tracker to update: 2.0700323581695557\n",
      "Time required to draw results for each track: 2.0700323581695557\n",
      "Time required to align video from: 0.01756429672241211\n",
      "Time required to predict: 1.4315924644470215\n",
      "Time required to encode: 2.1612324714660645\n",
      "Time required to run non maxima suppression: 2.1612324714660645\n",
      "Time required for tracker to update: 2.1638216972351074\n",
      "Time required to draw results for each track: 2.1638216972351074\n",
      "Time required to align video from: 0.017032146453857422\n",
      "Time required to predict: 1.3570294380187988\n",
      "Time required to encode: 1.9550292491912842\n",
      "Time required to run non maxima suppression: 1.9550292491912842\n",
      "Time required for tracker to update: 1.9570298194885254\n",
      "Time required to draw results for each track: 1.9570298194885254\n",
      "Time required to align video from: 0.016025781631469727\n",
      "Time required to predict: 1.499105453491211\n",
      "Time required to encode: 2.1131670475006104\n",
      "Time required to run non maxima suppression: 2.1131670475006104\n",
      "Time required for tracker to update: 2.1141417026519775\n",
      "Time required to draw results for each track: 2.1141417026519775\n",
      "Time required to align video from: 0.017002105712890625\n",
      "Time required to predict: 1.4224638938903809\n",
      "Time required to encode: 2.0894579887390137\n",
      "Time required to run non maxima suppression: 2.0894579887390137\n",
      "Time required for tracker to update: 2.0914926528930664\n",
      "Time required to draw results for each track: 2.0914926528930664\n"
     ]
    }
   ],
   "source": [
    "from absl import flags\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from _collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "from yolov3_tf2.models import YoloV3\n",
    "from yolov3_tf2.dataset import transform_images\n",
    "from yolov3_tf2.utils import convert_boxes\n",
    "\n",
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "\n",
    "from tools import generate_detections as gdet\n",
    "\n",
    "# checks if video is available\n",
    "def check_video_present(video):\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        print ('Cannot read video file')\n",
    "        sys.exit()\n",
    "    return frame\n",
    "\n",
    "# create file for video output\n",
    "def get_output_video(vid):\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID') # avi format\n",
    "    vid_fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    vid_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    return cv2.VideoWriter('./data/video/output.avi', codec, vid_fps, vid_size)\n",
    "\n",
    "# align video to the model dimensions\n",
    "def align_video_to_model(img):\n",
    "    # convert color space from BGR to RGB because YOLOv3 was trained on RGB images\n",
    "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # expand the image to have a batch dimension\n",
    "    img_in = tf.expand_dims(img_in, 0)\n",
    "\n",
    "    # resize the image to the input size of the model, i.e. 416x416 pixels for YOLOv3\n",
    "    img_in = transform_images(img_in, 416)\n",
    "\n",
    "    return img_in\n",
    "\n",
    "# read coco names based on IDs\n",
    "def get_class_coco_names(classes):\n",
    "    names = []\n",
    "    for i in range(len(classes)):\n",
    "        names.append(class_names[int(classes[i])])\n",
    "    return np.array(names)\n",
    "  \n",
    "# get region of interest\n",
    "def get_region_of_interest_selection(frame):\n",
    "    roi_coordinates = cv2.selectROI(frame, False)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Region of interest: \", roi_coordinates)\n",
    "\n",
    "    return roi_coordinates\n",
    "\n",
    "# apply non-max suppression to the bounding boxes\n",
    "def run_non_maxima_suppression(detections):\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "\n",
    "    # indices of the kept boxes, eliminated multi frame detections\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    return [detections[i] for i in indices]\n",
    "\n",
    "# Set the flags for the model\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv[:1])\n",
    "\n",
    "# Enforce tensorflow v1\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# CONFIG\n",
    "output_video = False\n",
    "max_cosine_distance = 0.5       # is it the same object?\n",
    "nn_budget = None                # number of features to be stored in the memory\n",
    "nms_max_overlap = 0.8           # non-maxima suppression, i.e. removes all boxes with a lower score than the max box\n",
    "model_filename = 'model_data/mars-small128.pb'          # pre-trained model for pedestrian tracking\n",
    "\n",
    "# Variable Section\n",
    "class_names = [c.strip() for c in open('./data/labels/coco.names').readlines()]\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0,1,20)]\n",
    "pts = [deque(maxlen=30) for _ in range(1000)]       # 1000 is the maximum number of objects to be tracked, here we use 30 points to draw the trajectory\n",
    "counter = []\n",
    "\n",
    "# load video\n",
    "vid = cv2.VideoCapture('./data/video/los_angeles.mp4')\n",
    "frame = check_video_present(vid)\n",
    "roi = get_region_of_interest_selection(frame)\n",
    "out = get_output_video(vid)\n",
    "\n",
    "# initialize yolo\n",
    "yolo = YoloV3(classes=len(class_names))\n",
    "yolo.load_weights('./weights/yolov3.tf')\n",
    "#yolo = YoloV3Tiny(classes=len(class_names))\n",
    "#yolo.load_weights('./weights/yolov3-tiny.tf')\n",
    "\n",
    "# initialaize encoder\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)\n",
    "\n",
    "while True:\n",
    "    _, img = vid.read()\n",
    "    if img is None:\n",
    "        print('Completed')\n",
    "        break\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    img_in = align_video_to_model(img)\n",
    "    print(\"Time required to align video from: \" + str(time.time()-t1))\n",
    "\n",
    "    # object detection using YOLO\n",
    "    # boxes 3D shape: (1, 100, 4)\n",
    "    # scores 2D shape: (1, 100)\n",
    "    # classes 2D shape: (1, 100)\n",
    "    # nums 1D shape: (1,)\n",
    "    \n",
    "    boxes, scores, classes, nums = yolo.predict(img_in, steps=1)\n",
    "    print(\"Time required to predict: \" + str(time.time()-t1))\n",
    "    classes = classes[0]\n",
    "\n",
    "    # get the bounding boxes of detected objects\n",
    "    converted_boxes = convert_boxes(img, boxes[0])\n",
    "\n",
    "    # get the feature vectors of the detected objects\n",
    "    features = encoder(img, converted_boxes)\n",
    "    print(\"Time required to encode: \" + str(time.time()-t1))\n",
    "\n",
    "    # initialize detections\n",
    "    detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(converted_boxes, scores[0], classes, features)]\n",
    "    detections = run_non_maxima_suppression(detections)\n",
    "    print(\"Time required to run non maxima suppression: \" + str(time.time()-t1))\n",
    "\n",
    "    # execute kalman filter\n",
    "    tracker.predict()\n",
    "    tracker.update(detections)\n",
    "    print(\"Time required for tracker to update: \" + str(time.time()-t1))\n",
    "\n",
    "    current_count = int(0)\n",
    "    for track in tracker.tracks:\n",
    "        # if kalman has no update, skip\n",
    "        if not track.is_confirmed() or track.time_since_update >1:\n",
    "            continue\n",
    "\n",
    "        bbox = track.to_tlbr()\n",
    "        class_name= class_names[int(track.get_class())]\n",
    "        color = colors[int(track.get_class()) % len(colors)]            # color of the bounding box\n",
    "        color = [i * 255 for i in color]                                # convert to RGB\n",
    "\n",
    "        # draw bounding box\n",
    "        cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), color, 2)\n",
    "        \n",
    "        # draw label with class name and track id\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name) + len(str(track.track_id))) * 17, int(bbox[1])), color, -1)\n",
    "        cv2.putText(img, class_name + \"-\" + str(track.track_id), (int(bbox[0]), int(bbox[1] - 10)), 0, 0.75,(255, 255, 255), 2)\n",
    "\n",
    "        # draw trajectory\n",
    "        center = (int(((bbox[0]) + (bbox[2])) / 2), int(((bbox[1]) + (bbox[3])) / 2))\n",
    "        pts[track.track_id].append(center)\n",
    "\n",
    "        for j in range(1, len(pts[track.track_id])):\n",
    "            # if we do not have enough points to draw a line, skip\n",
    "            if pts[track.track_id][j] is None or  pts[track.track_id][j-1] is None:\n",
    "                continue\n",
    "\n",
    "            thickness = int(np.sqrt(64/float(j+1))*2)       # thickness of the line is inversely proportional to the number of points\n",
    "            cv2.line(img, (pts[track.track_id][j-1]), (pts[track.track_id][j]), color, thickness)\n",
    "\n",
    "        \n",
    "        # count the number of objects in the ROI\n",
    "        height, width, _ = img.shape\n",
    "        cv2.line(img, (0, int(3*height/6+height/20)), (width, int(3*height/6+height/20)), (0, 255, 0), thickness=2)\n",
    "        cv2.line(img, (0, int(3*height/6-height/20)), (width, int(3*height/6-height/20)), (0, 255, 0), thickness=2)\n",
    "\n",
    "        center_y = int(((bbox[1])+(bbox[3]))/2)\n",
    "\n",
    "        if center_y <= int(3*height/6+height/20) and center_y >= int(3*height/6-height/20):\n",
    "            if class_name == 'car' or class_name == 'truck':\n",
    "                counter.append(int(track.track_id))\n",
    "                current_count += 1\n",
    "\n",
    "    print(\"Time required to draw results for each track: \" + str(time.time()-t1))\n",
    "    \n",
    "    total_count = len(set(counter))\n",
    "    cv2.putText(img, \"Current Vehicle Count: \" + str(current_count), (0, 80), 0, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(img, \"Total Vehicle Count: \" + str(total_count), (0,130), 0, 1, (0,0,255), 2)\n",
    "\n",
    "    # draw FPS\n",
    "    fps = 1./(time.time()-t1)\n",
    "    cv2.putText(img, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "\n",
    "    cv2.imshow('output', img)\n",
    "    cv2.resizeWindow('output', 1024, 768)\n",
    "    \n",
    "    if output_video:\n",
    "        out.write(img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('opencv-tracking-v4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6855a98cfab7939b13b212533748f76747928d06a090b83bc1c6f9d02b96c704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
