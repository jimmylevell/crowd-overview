{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Detection and Tracking\n",
    "Based on: https://github.com/emasterclassacademy/Single-Multiple-Custom-Object-Detection-and-Tracking\n",
    "\n",
    "Using OpenCV and EuclidanDistTracker. Machine Learning approach. \n",
    "\n",
    "Object Detection - YOLO\n",
    "- Will be applied to each and every frame\n",
    "\n",
    "Object Tracking - DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tf_cuda(): \n",
    "    import tensorflow as tf\n",
    "    return len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "def check_cv2_cuda():\n",
    "    import cv2\n",
    "    import re\n",
    "    cv_info = [re.sub('\\s+', ' ', ci.strip()) for ci in cv2.getBuildInformation().strip().split('\\n')\n",
    "                if len(ci) > 0 and re.search(r'(nvidia*:?)|(cuda*:)|(cudnn*:)', ci.lower()) is not None]\n",
    "\n",
    "    return len(cv_info) > 0\n",
    "\n",
    "def download_weights():\n",
    "    # download weights of not present\n",
    "    import urllib.request\n",
    "    import os\n",
    "    if not os.path.exists(\"weights/yolov3.weights\"):\n",
    "        print(\"Downloading weights...\")\n",
    "        urllib.request.urlretrieve(settings.weight_urls, \"weights/yolov3.weights\")\n",
    "    else:\n",
    "        print(\"Weights already downloaded\")\n",
    "\n",
    "    if not os.path.exists(\"weights/yolov3-tiny.weights\"):\n",
    "        print(\"Downloading tiny weights...\")\n",
    "        urllib.request.urlretrieve(settings.tiny_weight_urls, \"weights/yolov3-tiny.weights\")\n",
    "    else:\n",
    "        print(\"Tiny Weights already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU is available\n",
      "CV2 GPU is available\n",
      "Downloading weights...\n",
      "Downloading tiny weights...\n",
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " yolo_darknet (Functional)      ((None, None, None,  40620640    ['input[0][0]']                  \n",
      "                                 256),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 512),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 1024))                                                           \n",
      "                                                                                                  \n",
      " yolo_conv_0 (Functional)       (None, None, None,   11024384    ['yolo_darknet[0][2]']           \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " yolo_conv_1 (Functional)       (None, None, None,   2957312     ['yolo_conv_0[0][0]',            \n",
      "                                256)                              'yolo_darknet[0][1]']           \n",
      "                                                                                                  \n",
      " yolo_conv_2 (Functional)       (None, None, None,   741376      ['yolo_conv_1[0][0]',            \n",
      "                                128)                              'yolo_darknet[0][0]']           \n",
      "                                                                                                  \n",
      " yolo_output_0 (Functional)     (None, None, None,   4984063     ['yolo_conv_0[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_1 (Functional)     (None, None, None,   1312511     ['yolo_conv_1[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_2 (Functional)     (None, None, None,   361471      ['yolo_conv_2[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_boxes_0 (Lambda)          ((None, None, None,  0           ['yolo_output_0[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_1 (Lambda)          ((None, None, None,  0           ['yolo_output_1[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_2 (Lambda)          ((None, None, None,  0           ['yolo_output_2[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_nms (Lambda)              ((None, 100, 4),     0           ['yolo_boxes_0[0][0]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][1]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][2]',           \n",
      "                                 (None,))                         'yolo_boxes_1[0][0]',           \n",
      "                                                                  'yolo_boxes_1[0][1]',           \n",
      "                                                                  'yolo_boxes_1[0][2]',           \n",
      "                                                                  'yolo_boxes_2[0][0]',           \n",
      "                                                                  'yolo_boxes_2[0][1]',           \n",
      "                                                                  'yolo_boxes_2[0][2]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "model created\n",
      "weights loaded\n",
      "sanity check passed\n",
      "weights saved\n",
      "Model: \"yolov3_tiny\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " yolo_darknet (Functional)      ((None, None, None,  6298480     ['input[0][0]']                  \n",
      "                                 256),                                                            \n",
      "                                 (None, None, None,                                               \n",
      "                                 1024))                                                           \n",
      "                                                                                                  \n",
      " yolo_conv_0 (Functional)       (None, None, None,   263168      ['yolo_darknet[0][1]']           \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " yolo_conv_1 (Functional)       (None, None, None,   33280       ['yolo_conv_0[0][0]',            \n",
      "                                384)                              'yolo_darknet[0][0]']           \n",
      "                                                                                                  \n",
      " yolo_output_0 (Functional)     (None, None, None,   1312511     ['yolo_conv_0[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_output_1 (Functional)     (None, None, None,   951295      ['yolo_conv_1[0][0]']            \n",
      "                                3, 85)                                                            \n",
      "                                                                                                  \n",
      " yolo_boxes_0 (Lambda)          ((None, None, None,  0           ['yolo_output_0[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_boxes_1 (Lambda)          ((None, None, None,  0           ['yolo_output_1[0][0]']          \n",
      "                                 3, 4),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 1),                                                           \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 80),                                                          \n",
      "                                 (None, None, None,                                               \n",
      "                                 3, 4))                                                           \n",
      "                                                                                                  \n",
      " yolo_nms (Lambda)              ((None, 100, 4),     0           ['yolo_boxes_0[0][0]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][1]',           \n",
      "                                 (None, 100),                     'yolo_boxes_0[0][2]',           \n",
      "                                 (None,))                         'yolo_boxes_1[0][0]',           \n",
      "                                                                  'yolo_boxes_1[0][1]',           \n",
      "                                                                  'yolo_boxes_1[0][2]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,858,734\n",
      "Trainable params: 8,852,366\n",
      "Non-trainable params: 6,368\n",
      "__________________________________________________________________________________________________\n",
      "model created\n",
      "weights loaded\n",
      "sanity check passed\n",
      "weights saved\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import settings\n",
    "\n",
    "settings.init()\n",
    "\n",
    "# Check if we have a GPU support TF\n",
    "if check_tf_cuda():\n",
    "    print(\"TensorFlow GPU is available\")\n",
    "else:\n",
    "    print(\"TensorFlow GPU is NOT available\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check if we have a GPU support OpenCV\n",
    "if check_cv2_cuda():\n",
    "    print(\"CV2 GPU is available\")\n",
    "else:\n",
    "    print(\"CV2 GPU is NOT available\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# check if weights are downloaded\n",
    "download_weights()\n",
    "\n",
    "# check if weights have been converted\n",
    "# Convert the weights to TensorFlow\n",
    "from convert import convert\n",
    "convert()\n",
    "convert(tiny=True, weights=\"weights/yolov3-tiny.weights\", output=\"weights/yolov3-tiny.tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region of interest:  (604, 397, 340, 154)\n",
      "Num GPUs Available:  1\n",
      "Time required to align video from: 0.015967369079589844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\.conda\\envs\\opencv-tracking-v4\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to predict: 1.205965518951416\n",
      "Time required to encode: 3.5091047286987305\n",
      "Time required to run non maxima suppression: 3.5101380348205566\n",
      "Time required for tracker to update: 3.51210618019104\n",
      "Time required to draw results for each track: 3.51210618019104\n",
      "Time required to align video from: 0.018034696578979492\n",
      "Time required to predict: 1.2107772827148438\n",
      "Time required to encode: 1.3187801837921143\n",
      "Time required to run non maxima suppression: 1.3207511901855469\n",
      "Time required for tracker to update: 1.3247802257537842\n",
      "Time required to draw results for each track: 1.3247802257537842\n",
      "Time required to align video from: 0.015844345092773438\n",
      "Time required to predict: 1.0759687423706055\n",
      "Time required to encode: 1.1379384994506836\n",
      "Time required to run non maxima suppression: 1.1389362812042236\n",
      "Time required for tracker to update: 1.1439342498779297\n",
      "Time required to draw results for each track: 1.144935131072998\n",
      "Time required to align video from: 0.019032716751098633\n",
      "Time required to predict: 1.097771167755127\n",
      "Time required to encode: 1.1727416515350342\n",
      "Time required to run non maxima suppression: 1.1737403869628906\n",
      "Time required for tracker to update: 1.1867380142211914\n",
      "Time required to draw results for each track: 1.188772201538086\n",
      "Time required to align video from: 0.015990257263183594\n",
      "Time required to predict: 1.0716018676757812\n",
      "Time required to encode: 1.137695550918579\n",
      "Time required to run non maxima suppression: 1.1386959552764893\n",
      "Time required for tracker to update: 1.1476938724517822\n",
      "Time required to draw results for each track: 1.1497175693511963\n",
      "Time required to align video from: 0.019460201263427734\n",
      "Time required to predict: 1.1066014766693115\n",
      "Time required to encode: 1.1665716171264648\n",
      "Time required to run non maxima suppression: 1.1675686836242676\n",
      "Time required for tracker to update: 1.175569772720337\n",
      "Time required to draw results for each track: 1.1775684356689453\n",
      "Time required to align video from: 0.015997886657714844\n",
      "Time required to predict: 1.0848877429962158\n",
      "Time required to encode: 1.1488564014434814\n",
      "Time required to run non maxima suppression: 1.1488564014434814\n",
      "Time required for tracker to update: 1.160856008529663\n",
      "Time required to draw results for each track: 1.1618552207946777\n",
      "Time required to align video from: 0.01888728141784668\n",
      "Time required to predict: 1.1011817455291748\n",
      "Time required to encode: 1.1541485786437988\n",
      "Time required to run non maxima suppression: 1.155149221420288\n",
      "Time required for tracker to update: 1.1661491394042969\n",
      "Time required to draw results for each track: 1.1671500205993652\n",
      "Time required to align video from: 0.017763853073120117\n",
      "Time required to predict: 1.1098389625549316\n",
      "Time required to encode: 1.1658096313476562\n",
      "Time required to run non maxima suppression: 1.166808843612671\n",
      "Time required for tracker to update: 1.176806926727295\n",
      "Time required to draw results for each track: 1.1778056621551514\n",
      "Time required to align video from: 0.015995502471923828\n",
      "Time required to predict: 1.148496150970459\n",
      "Time required to encode: 1.2135133743286133\n",
      "Time required to run non maxima suppression: 1.2135133743286133\n",
      "Time required for tracker to update: 1.2255122661590576\n",
      "Time required to draw results for each track: 1.226511001586914\n",
      "Time required to align video from: 0.016999483108520508\n",
      "Time required to predict: 1.1471269130706787\n",
      "Time required to encode: 1.2112739086151123\n",
      "Time required to run non maxima suppression: 1.2112739086151123\n",
      "Time required for tracker to update: 1.223271369934082\n",
      "Time required to draw results for each track: 1.224271535873413\n",
      "Time required to align video from: 0.017029523849487305\n",
      "Time required to predict: 1.116894006729126\n",
      "Time required to encode: 1.176861047744751\n",
      "Time required to run non maxima suppression: 1.177863597869873\n",
      "Time required for tracker to update: 1.190861701965332\n",
      "Time required to draw results for each track: 1.1928610801696777\n",
      "Time required to align video from: 0.016002893447875977\n",
      "Time required to predict: 1.119934320449829\n",
      "Time required to encode: 1.1790294647216797\n",
      "Time required to run non maxima suppression: 1.1799440383911133\n",
      "Time required for tracker to update: 1.1939175128936768\n",
      "Time required to draw results for each track: 1.1949453353881836\n",
      "Time required to align video from: 0.016016483306884766\n",
      "Time required to predict: 1.1424190998077393\n",
      "Time required to encode: 1.2114291191101074\n",
      "Time required to run non maxima suppression: 1.212418794631958\n",
      "Time required for tracker to update: 1.2283856868743896\n",
      "Time required to draw results for each track: 1.230386734008789\n",
      "Time required to align video from: 0.016030550003051758\n",
      "Time required to predict: 1.2454845905303955\n",
      "Time required to encode: 1.3184864521026611\n",
      "Time required to run non maxima suppression: 1.3194856643676758\n",
      "Time required for tracker to update: 1.3374874591827393\n",
      "Time required to draw results for each track: 1.3394854068756104\n",
      "Time required to align video from: 0.01697397232055664\n",
      "Time required to predict: 1.161651372909546\n",
      "Time required to encode: 1.2126569747924805\n",
      "Time required to run non maxima suppression: 1.2126569747924805\n",
      "Time required for tracker to update: 1.2286510467529297\n",
      "Time required to draw results for each track: 1.2306530475616455\n",
      "Time required to align video from: 0.015999555587768555\n",
      "Time required to predict: 1.186023473739624\n",
      "Time required to encode: 1.2450203895568848\n",
      "Time required to run non maxima suppression: 1.246023416519165\n",
      "Time required for tracker to update: 1.2610208988189697\n",
      "Time required to draw results for each track: 1.26302170753479\n",
      "Time required to align video from: 0.01599860191345215\n",
      "Time required to predict: 1.1774065494537354\n",
      "Time required to encode: 1.2254061698913574\n",
      "Time required to run non maxima suppression: 1.2254061698913574\n",
      "Time required for tracker to update: 1.243398904800415\n",
      "Time required to draw results for each track: 1.244398593902588\n",
      "Time required to align video from: 0.015020132064819336\n",
      "Time required to predict: 1.1619887351989746\n",
      "Time required to encode: 1.2160184383392334\n",
      "Time required to run non maxima suppression: 1.2160184383392334\n",
      "Time required for tracker to update: 1.2329859733581543\n",
      "Time required to draw results for each track: 1.2349858283996582\n",
      "Time required to align video from: 0.015991687774658203\n",
      "Time required to predict: 1.1716229915618896\n",
      "Time required to encode: 1.226647138595581\n",
      "Time required to run non maxima suppression: 1.2276477813720703\n",
      "Time required for tracker to update: 1.2436211109161377\n",
      "Time required to draw results for each track: 1.2456214427947998\n",
      "Time required to align video from: 0.016026973724365234\n",
      "Time required to predict: 1.1784427165985107\n",
      "Time required to encode: 1.225409984588623\n",
      "Time required to run non maxima suppression: 1.2264435291290283\n",
      "Time required for tracker to update: 1.2410266399383545\n",
      "Time required to draw results for each track: 1.2430293560028076\n",
      "Time required to align video from: 0.01599860191345215\n",
      "Time required to predict: 1.24072265625\n",
      "Time required to encode: 1.3056895732879639\n",
      "Time required to run non maxima suppression: 1.3066904544830322\n",
      "Time required for tracker to update: 1.3222837448120117\n",
      "Time required to draw results for each track: 1.3252925872802734\n",
      "Time required to align video from: 0.017001867294311523\n",
      "Time required to predict: 1.2246181964874268\n",
      "Time required to encode: 1.2926278114318848\n",
      "Time required to run non maxima suppression: 1.293626070022583\n",
      "Time required for tracker to update: 1.3095927238464355\n",
      "Time required to draw results for each track: 1.310594081878662\n",
      "Time required to align video from: 0.01697397232055664\n",
      "Time required to predict: 1.2440097332000732\n",
      "Time required to encode: 1.3089869022369385\n",
      "Time required to run non maxima suppression: 1.3100173473358154\n",
      "Time required for tracker to update: 1.323988676071167\n",
      "Time required to draw results for each track: 1.3260176181793213\n",
      "Time required to align video from: 0.0160367488861084\n",
      "Time required to predict: 1.237203598022461\n",
      "Time required to encode: 1.2962076663970947\n",
      "Time required to run non maxima suppression: 1.2972087860107422\n",
      "Time required for tracker to update: 1.3129489421844482\n",
      "Time required to draw results for each track: 1.314957618713379\n",
      "Time required to align video from: 0.016031265258789062\n",
      "Time required to predict: 1.2419278621673584\n",
      "Time required to encode: 1.3049359321594238\n",
      "Time required to run non maxima suppression: 1.3059017658233643\n",
      "Time required for tracker to update: 1.3238966464996338\n",
      "Time required to draw results for each track: 1.3258945941925049\n",
      "Time required to align video from: 0.016027212142944336\n",
      "Time required to predict: 1.2901074886322021\n",
      "Time required to encode: 1.3551335334777832\n",
      "Time required to run non maxima suppression: 1.3561019897460938\n",
      "Time required for tracker to update: 1.3751013278961182\n",
      "Time required to draw results for each track: 1.377101182937622\n",
      "Time required to align video from: 0.016995668411254883\n",
      "Time required to predict: 1.4095478057861328\n",
      "Time required to encode: 1.4669826030731201\n",
      "Time required to run non maxima suppression: 1.467513084411621\n",
      "Time required for tracker to update: 1.4842000007629395\n",
      "Time required to draw results for each track: 1.4862499237060547\n",
      "Time required to align video from: 0.015530586242675781\n",
      "Time required to predict: 1.4320704936981201\n",
      "Time required to encode: 1.4890720844268799\n",
      "Time required to run non maxima suppression: 1.490072250366211\n",
      "Time required for tracker to update: 1.506098747253418\n",
      "Time required to draw results for each track: 1.50809645652771\n",
      "Time required to align video from: 0.014995098114013672\n",
      "Time required to predict: 1.2777581214904785\n",
      "Time required to encode: 1.34275484085083\n",
      "Time required to run non maxima suppression: 1.3437581062316895\n",
      "Time required for tracker to update: 1.3597540855407715\n",
      "Time required to draw results for each track: 1.3617582321166992\n",
      "Time required to align video from: 0.021999120712280273\n",
      "Time required to predict: 1.2717628479003906\n",
      "Time required to encode: 1.331730842590332\n",
      "Time required to run non maxima suppression: 1.331730842590332\n",
      "Time required for tracker to update: 1.348752737045288\n",
      "Time required to draw results for each track: 1.350752353668213\n",
      "Time required to align video from: 0.014986991882324219\n",
      "Time required to predict: 1.294865369796753\n",
      "Time required to encode: 1.3528327941894531\n",
      "Time required to run non maxima suppression: 1.3538312911987305\n",
      "Time required for tracker to update: 1.373629093170166\n",
      "Time required to draw results for each track: 1.3755993843078613\n",
      "Time required to align video from: 0.016999483108520508\n",
      "Time required to predict: 1.289229154586792\n",
      "Time required to encode: 1.3452248573303223\n",
      "Time required to run non maxima suppression: 1.3452248573303223\n",
      "Time required for tracker to update: 1.3652245998382568\n",
      "Time required to draw results for each track: 1.367224931716919\n",
      "Time required to align video from: 0.016158103942871094\n",
      "Time required to predict: 1.2648797035217285\n",
      "Time required to encode: 1.3246126174926758\n",
      "Time required to run non maxima suppression: 1.3256139755249023\n",
      "Time required for tracker to update: 1.343581199645996\n",
      "Time required to draw results for each track: 1.346580982208252\n",
      "Time required to align video from: 0.01599597930908203\n",
      "Time required to predict: 1.2968635559082031\n",
      "Time required to encode: 1.3528332710266113\n",
      "Time required to run non maxima suppression: 1.353832483291626\n",
      "Time required for tracker to update: 1.3728306293487549\n",
      "Time required to draw results for each track: 1.374831199645996\n",
      "Time required to align video from: 0.01599717140197754\n",
      "Time required to predict: 1.350268840789795\n",
      "Time required to encode: 1.4162695407867432\n",
      "Time required to run non maxima suppression: 1.4162695407867432\n",
      "Time required for tracker to update: 1.4382762908935547\n",
      "Time required to draw results for each track: 1.440300464630127\n",
      "Time required to align video from: 0.01800084114074707\n",
      "Time required to predict: 1.3578505516052246\n",
      "Time required to encode: 1.4228487014770508\n",
      "Time required to run non maxima suppression: 1.4228487014770508\n",
      "Time required for tracker to update: 1.4408502578735352\n",
      "Time required to draw results for each track: 1.4438486099243164\n",
      "Time required to align video from: 0.016966819763183594\n",
      "Time required to predict: 1.3602514266967773\n",
      "Time required to encode: 1.4262473583221436\n",
      "Time required to run non maxima suppression: 1.427248239517212\n",
      "Time required for tracker to update: 1.4432530403137207\n",
      "Time required to draw results for each track: 1.4462525844573975\n",
      "Time required to align video from: 0.019125699996948242\n",
      "Time required to predict: 1.3654303550720215\n",
      "Time required to encode: 1.4344286918640137\n",
      "Time required to run non maxima suppression: 1.4354071617126465\n",
      "Time required for tracker to update: 1.4544072151184082\n",
      "Time required to draw results for each track: 1.4574074745178223\n"
     ]
    }
   ],
   "source": [
    "from absl import flags\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from _collections import deque\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the model\n",
    "from yolov3_tf2.models import YoloV3\n",
    "from yolov3_tf2.dataset import transform_images\n",
    "from yolov3_tf2.utils import convert_boxes\n",
    "\n",
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "\n",
    "from tools import generate_detections as gdet\n",
    "\n",
    "# checks if video is available\n",
    "def check_video_present(video):\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        print ('Cannot read video file')\n",
    "        sys.exit()\n",
    "    return frame\n",
    "\n",
    "# create file for video output\n",
    "def get_output_video(vid):\n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID') # avi format\n",
    "    vid_fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    vid_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    return cv2.VideoWriter('./data/video/output.avi', codec, vid_fps, vid_size)\n",
    "\n",
    "# align video to the model dimensions\n",
    "def align_video_to_model(img):\n",
    "    # convert color space from BGR to RGB because YOLOv3 was trained on RGB images\n",
    "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # expand the image to have a batch dimension\n",
    "    img_in = tf.expand_dims(img_in, 0)\n",
    "\n",
    "    # resize the image to the input size of the model, i.e. 416x416 pixels for YOLOv3\n",
    "    img_in = transform_images(img_in, input_size)\n",
    "\n",
    "    return img_in\n",
    "\n",
    "# read coco names based on IDs\n",
    "def get_class_coco_names(classes):\n",
    "    names = []\n",
    "    for i in range(len(classes)):\n",
    "        names.append(class_names[int(classes[i])])\n",
    "    return np.array(names)\n",
    "  \n",
    "# get region of interest\n",
    "def get_region_of_interest_selection(frame):\n",
    "    roi_coordinates = cv2.selectROI(frame, False)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Region of interest: \", roi_coordinates)\n",
    "\n",
    "    return roi_coordinates\n",
    "\n",
    "# apply non-max suppression to the bounding boxes\n",
    "def run_non_maxima_suppression(detections):\n",
    "    boxs = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    classes = np.array([d.class_name for d in detections])\n",
    "\n",
    "    # indices of the kept boxes, eliminated multi frame detections\n",
    "    indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "    return [detections[i] for i in indices]\n",
    "\n",
    "# Set the flags for the model\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(sys.argv[:1])\n",
    "\n",
    "# Enforce tensorflow v1\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# CONFIG\n",
    "input_size = 288  # 416\n",
    "output_video = False\n",
    "max_cosine_distance = 0.5       # is it the same object?\n",
    "nn_budget = None                # number of features to be stored in the memory\n",
    "nms_max_overlap = 0.8           # non-maxima suppression, i.e. removes all boxes with a lower score than the max box\n",
    "model_filename = 'model_data/mars-small128.pb'          # pre-trained model for pedestrian tracking\n",
    "\n",
    "# Variable Section\n",
    "class_names = [c.strip() for c in open('./data/labels/coco.names').readlines()]\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0,1,20)]\n",
    "pts = [deque(maxlen=30) for _ in range(1000)]       # 1000 is the maximum number of objects to be tracked, here we use 30 points to draw the trajectory\n",
    "counter = []\n",
    "\n",
    "# load video\n",
    "vid = cv2.VideoCapture('./data/video/los_angeles.mp4')\n",
    "frame = check_video_present(vid)\n",
    "roi = get_region_of_interest_selection(frame)\n",
    "out = get_output_video(vid)\n",
    "\n",
    "# initialaize encoder\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric('cosine', max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        K.set_session(sess)\n",
    "\n",
    "        yolo = YoloV3(classes=len(class_names), size=input_size)\n",
    "        yolo.load_weights('./weights/yolov3.tf')\n",
    "        #yolo = YoloV3Tiny(classes=len(class_names))\n",
    "        #yolo.load_weights('./weights/yolov3-tiny.tf')\n",
    "\n",
    "        while True:\n",
    "            _, img = vid.read()\n",
    "            if img is None:\n",
    "                print('Completed')\n",
    "                break\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            img_in = align_video_to_model(img)\n",
    "            print(\"Time required to align video from: \" + str(time.time()-t1))\n",
    "\n",
    "            # object detection using YOLO\n",
    "            # boxes 3D shape: (1, 100, 4)\n",
    "            # scores 2D shape: (1, 100)\n",
    "            # classes 2D shape: (1, 100)\n",
    "            # nums 1D shape: (1,)\n",
    "            \n",
    "            boxes, scores, classes, nums = yolo.predict(img_in, steps=1)\n",
    "            print(\"Time required to predict: \" + str(time.time()-t1))\n",
    "            classes = classes[0]\n",
    "\n",
    "            # get the bounding boxes of detected objects\n",
    "            converted_boxes = convert_boxes(img, boxes[0])\n",
    "\n",
    "            # get the feature vectors of the detected objects\n",
    "            features = encoder(img, converted_boxes)\n",
    "            print(\"Time required to encode: \" + str(time.time()-t1))\n",
    "\n",
    "            # initialize detections\n",
    "            detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(converted_boxes, scores[0], classes, features)]\n",
    "            detections = run_non_maxima_suppression(detections)\n",
    "            print(\"Time required to run non maxima suppression: \" + str(time.time()-t1))\n",
    "\n",
    "            # execute kalman filter\n",
    "            tracker.predict()\n",
    "            tracker.update(detections)\n",
    "            print(\"Time required for tracker to update: \" + str(time.time()-t1))\n",
    "\n",
    "            current_count = int(0)\n",
    "            for track in tracker.tracks:\n",
    "                # if kalman has no update, skip\n",
    "                if not track.is_confirmed() or track.time_since_update >1:\n",
    "                    continue\n",
    "\n",
    "                bbox = track.to_tlbr()\n",
    "                class_name= class_names[int(track.get_class())]\n",
    "                color = colors[int(track.get_class()) % len(colors)]            # color of the bounding box\n",
    "                color = [i * 255 for i in color]                                # convert to RGB\n",
    "\n",
    "                # draw bounding box\n",
    "                cv2.rectangle(img, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), color, 2)\n",
    "                \n",
    "                # draw label with class name and track id\n",
    "                cv2.rectangle(img, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name) + len(str(track.track_id))) * 17, int(bbox[1])), color, -1)\n",
    "                cv2.putText(img, class_name + \"-\" + str(track.track_id), (int(bbox[0]), int(bbox[1] - 10)), 0, 0.75,(255, 255, 255), 2)\n",
    "\n",
    "                # draw trajectory\n",
    "                center = (int(((bbox[0]) + (bbox[2])) / 2), int(((bbox[1]) + (bbox[3])) / 2))\n",
    "                pts[track.track_id].append(center)\n",
    "\n",
    "                for j in range(1, len(pts[track.track_id])):\n",
    "                    # if we do not have enough points to draw a line, skip\n",
    "                    if pts[track.track_id][j] is None or  pts[track.track_id][j-1] is None:\n",
    "                        continue\n",
    "\n",
    "                    thickness = int(np.sqrt(64/float(j+1))*2)       # thickness of the line is inversely proportional to the number of points\n",
    "                    cv2.line(img, (pts[track.track_id][j-1]), (pts[track.track_id][j]), color, thickness)\n",
    "\n",
    "                \n",
    "                # count the number of objects in the ROI\n",
    "                height, width, _ = img.shape\n",
    "                cv2.line(img, (0, int(3*height/6+height/20)), (width, int(3*height/6+height/20)), (0, 255, 0), thickness=2)\n",
    "                cv2.line(img, (0, int(3*height/6-height/20)), (width, int(3*height/6-height/20)), (0, 255, 0), thickness=2)\n",
    "\n",
    "                center_y = int(((bbox[1])+(bbox[3]))/2)\n",
    "\n",
    "                if center_y <= int(3*height/6+height/20) and center_y >= int(3*height/6-height/20):\n",
    "                    if class_name == 'car' or class_name == 'truck':\n",
    "                        counter.append(int(track.track_id))\n",
    "                        current_count += 1\n",
    "\n",
    "            print(\"Time required to draw results for each track: \" + str(time.time()-t1))\n",
    "            \n",
    "            total_count = len(set(counter))\n",
    "            cv2.putText(img, \"Current Vehicle Count: \" + str(current_count), (0, 80), 0, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(img, \"Total Vehicle Count: \" + str(total_count), (0,130), 0, 1, (0,0,255), 2)\n",
    "\n",
    "            # draw FPS\n",
    "            fps = 1./(time.time()-t1)\n",
    "            cv2.putText(img, \"FPS: {:.2f}\".format(fps), (0,30), 0, 1, (0,0,255), 2)\n",
    "\n",
    "            cv2.imshow('output', img)\n",
    "            cv2.resizeWindow('output', 1024, 768)\n",
    "            \n",
    "            if output_video:\n",
    "                out.write(img)\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                break\n",
    "\n",
    "        vid.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('opencv-tracking-v4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6855a98cfab7939b13b212533748f76747928d06a090b83bc1c6f9d02b96c704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
